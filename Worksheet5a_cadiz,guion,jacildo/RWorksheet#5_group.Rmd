---
title: "Worksheet#5a_group"
author: "Cadiz, Guion, Jacildo"
date: "2024-11-06"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# 1. Each group needs to extract the top 50 tv shows in Imdb.com. It will include the rank, the title of the tv show, tv rating, the number of people who voted, the number of episodes, the year it was released.

```{r}
library(rvest)  
library(httr)
library(dplyr)
library(polite)
library(kableExtra)
library(rmarkdown)

url_Imdb <- 'https://www.imdb.com/chart/toptv/?ref_=nv_tvv_250'

session <- bow(url_Imdb,
               user_agent = "Educational")
session
```
```{r}
library(rvest)
library(dplyr)

title <- read_html(url_Imdb) %>%
  html_nodes('h3.ipc-title__text') %>% 
  html_text


dataB <- data.frame(
    titleDf = title[2:26])

dataB
```
```{r}
title_rank <- as.data.frame(dataB, stringsAsFactors = FALSE)
colnames(title_rank) <- "rank"

split_df <- strsplit(as.character(title_rank$rank), "\\.", fixed = FALSE)
split_df <- data.frame(do.call(rbind, split_df), stringsAsFactors = FALSE)
colnames(split_df) <- c("rank", "title")

split_df <- split_df %>% select(rank, title)

split_df$title <- trimws(split_df$title)

title_rank <- split_df
title_rank
```

```{r}
rating <- read_html(url_Imdb) %>%
  html_nodes('.ipc-rating-star--rating') %>%
  html_text()

voted <- read_html(url_Imdb) %>%
  html_nodes('.ipc-rating-star--voteCount') %>%
  html_text()
  vot <- gsub('[()]', '', voted)


episodes <- read_html(url_Imdb) %>%
  html_nodes('span.sc-5bc66c50-6.OOdsw.cli-title-metadata-item:nth-of-type(2)') %>%
  html_text()
  ep1 <- gsub('[eps]', '', episodes)
  ep2 <- as.numeric(ep1)

years <- read_html(url_Imdb) %>%
  html_nodes('span.sc-5bc66c50-6.OOdsw.cli-title-metadata-item:nth-of-type(1)') %>%
  html_text()
```
```{r}
tv_shows <- data.frame(
  Rank = title_rank[,1],
  Title = title_rank[,2],
  Rating = rating,
  Voters = voted,
  Episodes = episodes,
  Year = years
)
tv_shows
```
# It will also include the number of user reviews and the number of critic reviews, as well as the popularity rating for each tv shows.
# Continuation of the project by - John Dave Cadiz
```{r}
home_link <- 'https://www.imdb.com/chart/toptv/'
main_page <- read_html(home_link)

links <- main_page %>%
  html_nodes("a.ipc-title-link-wrapper") %>%
  html_attr("href")

show_url_df <- do.call(rbind, lapply(links, function(link) {
  complete_link <- paste0("https://imdb.com", link)

 usrv_link <- read_html(complete_link)
  usrv_link_page <- usrv_link %>%
    html_nodes('a.isReview') %>%
    html_attr("href")
  
   critic <- usrv_link %>%
              html_nodes("span.score") %>%
              html_text()
  critic_df <- data.frame(Critic_Reviews = critic[2], stringsAsFactors = FALSE)
  
 pop_rating <- usrv_link %>%
              html_nodes('[data-testid="hero-rating-bar__popularity__score"]') %>%
              html_text()
  
   usrv <- read_html(paste0("https://imdb.com", usrv_link_page[1]))
  usrv_count <- usrv %>%
    html_nodes('[data-testid="tturv-total-reviews"]') %>%
    html_text()

 return(data.frame(Show_Link = complete_link, User_Reviews = usrv_count, Critic_Reviews = critic[2], Popularity_Rating = pop_rating)) 
}))

shows <- cbind(tv_shows, show_url_df)
shows
```
#chacha
2. From the 50 tv shows, select at least 5 tv shows to scrape 20 user reviews that will include the reviewer’s
name, date of reviewed, user rating, title of the review, the numbers for “is helpful” and “is not helpful”,
and text reviews.
```{r}
library(rvest)  
library(dplyr)

# List of IMDb URLs for 5 selected shows 
show_urls <- c(
  'https://www.imdb.com/title/tt0903747/reviews/?ref_=tt_ov_urv',   # Breaking Bad
  'https://www.imdb.com/title/tt5491994/reviews/?ref_=tt_ov_ql_2',  # Planet Earth II
  'https://www.imdb.com/title/tt0795176/reviews/?ref_=tt_ov_ql_2',  # Planet Earth
  'https://www.imdb.com/title/tt0185906/reviews/?ref_=tt_ov_ql_2',  # Band of Brothers
  'https://www.imdb.com/title/tt7366338/reviews/?ref_=tt_ov_ql_2'   # Chernobyl
)

scrape_reviews <- function(show_url) {
  
  review_page <- read_html(show_url)
  
  # Show name
  show_name <- review_page %>%
    html_nodes('h2') %>%      
    html_text() %>%
    trimws()

  # Reviewers' names
  reviewers <- review_page %>%
    html_nodes('a.ipc-link--base[data-testid="author-link"]') %>%
    html_text()

  # Review dates
  review_dates <- review_page %>%
    html_nodes('.review-date') %>%
    html_text()

  # User ratings
  user_ratings <- review_page %>%
    html_nodes('.ipc-rating-star--rating') %>%
    html_text() %>%
    as.numeric()

  # Review titles
  review_titles <- review_page %>%
    html_nodes('h3.ipc-title__text') %>%
    html_text()

  # Helpful count
  helpful_count <- review_page %>%
    html_nodes('.ipc-voting__label__count--up') %>%
    html_text() %>%
    as.numeric()

  # Not helpful count
  not_helpful_count <- review_page %>%
    html_nodes('.ipc-voting__label__count--down') %>%
    html_text() %>%
    as.numeric()

  # Review text
  review_text <- review_page %>%
    html_nodes('.ipc-html-content-inner-div') %>%
    html_text()
  
  review_text <- trimws(review_text) 

  # DataFrame
  reviews <- data.frame(
    Show = show_name,
    Reviewer = reviewers[1:20],
    Date = review_dates[1:20],
    UserRating = user_ratings[1:20],
    ReviewTitle = review_titles[1:20],
    HelpfulCount = helpful_count[1:20],
    NotHelpfulCount = not_helpful_count[1:20],
    ReviewText = review_text[1:20]
  )
  
  return(reviews)
}

all_reviews <- lapply(show_urls, scrape_reviews)
reviews_df <- bind_rows(all_reviews)
print(reviews_df)
```











# Extracting Amazon Product Reviews
# 4. Select 5 categories from Amazon and select 30 products from each category.
```{r}
library(rvest)
library(httr)
library(dplyr)
library(polite)
library(kableExtra)
library(rmarkdown)

url <- 'https://www.amazon.com/s?rh=n%3A3760911%2Cn%3A11058281&dc&qid=1730855869&rnid=3760911&ref=sr_nr_n_1i'

session <- bow(url,
               user_agent = "Educational")
session

```
# 5. Extract the price, description, ratings and reviews of each product.
```{r}
library(rvest)

page1 <- read_html(url)

price1 <- scrape(session) %>%
  html_nodes('.a-price .a-offscreen') %>% 
  html_text

data1 <- data.frame(
    priceDf1 = price1[1:30])

data1

description1 <- page1 %>% html_nodes('span.a-text-normal') %>% html_text()

data2 <- data.frame(
    desDf1 = description1[1:30])

data2

ratings1 <- scrape(session) %>%
  html_nodes('span.a-icon-alt') %>% 
  html_text

data3 <- data.frame(
    ratingDf1 = ratings1[1:30])

data3

reviews1 <- scrape(session) %>%
  html_nodes('.a-expander-partial-collapse-content') %>% 
  html_text

data4 <- data.frame(
    reviewsDf1 = reviews1[1:30])

data4

```
# 6. Describe the data you have extracted.
```{r}
library(psych)
describe(data1)
```


# 7. What will be your use case for the data you have extracted?
# 8. Create graphs regarding the use case. And briefly explain it.
# 9. Graph the price and the ratings for each category. Use basic plotting functions and ggplot2 package.
# 10. Rank the products of each category by price and ratings. Explain briefly
